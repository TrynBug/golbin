{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional Neural Network (DCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet code in TnesorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "VALIDATION_SPLIT=0.2\n",
    "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "NB_CLASSES = 10 # number of outputs = number of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the convnet(LeNet)\n",
    "def build(input_shape, classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # CONV -> RELU -> POOL\n",
    "    model.add(layers.Convolution2D(20, (5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    # CONV -> RELU -> POOL\n",
    "    model.add(layers.Convolution2D(50, (5,5), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    # Flatten -> RELU layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "    \n",
    "    # a softmax classifier\n",
    "    model.add(layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 24, 24, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 12, 12, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 8, 50)          25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 4, 4, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 500)               400500    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 42us/sample - loss: 0.2132 - accuracy: 0.9385 - val_loss: 0.0725 - val_accuracy: 0.9796\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0554 - accuracy: 0.9825 - val_loss: 0.0577 - val_accuracy: 0.9841\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.0396 - val_accuracy: 0.9882\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0451 - val_accuracy: 0.9871\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0367 - val_accuracy: 0.9905\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0162 - accuracy: 0.9947 - val_loss: 0.0431 - val_accuracy: 0.9873\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0415 - val_accuracy: 0.9891\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0452 - val_accuracy: 0.9892\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0393 - val_accuracy: 0.9898\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 27us/sample - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0394 - val_accuracy: 0.9892\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0413 - val_accuracy: 0.9902\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0371 - val_accuracy: 0.9918\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0450 - val_accuracy: 0.9898\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0492 - val_accuracy: 0.9887\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0504 - val_accuracy: 0.9878\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0432 - val_accuracy: 0.9911\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0451 - val_accuracy: 0.9894\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0471 - val_accuracy: 0.9900\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0485 - val_accuracy: 0.9900\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0484 - val_accuracy: 0.9908\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0379 - accuracy: 0.9916\n",
      "\n",
      "Test score: 0.03786451516075539\n",
      "Test accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# reshape\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# cast\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# convert class vectors to binary class matrices (one-hot encoding)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# initialize the optimizer and model\n",
    "model = build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "# use TensorBoard, princess Aurora!\n",
    "callbacks = [\n",
    "    # Write TensorBoard logs to './logs' directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "'''\n",
    "# fit\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "#                    ,callbacks=callbacks)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing CIFAR-10 images with deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "# constant\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = tf.keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the convnet\n",
    "def build(input_shape, classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Convolution2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "# normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# convert to categorical\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3686912   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,692,938\n",
      "Trainable params: 3,692,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "  128/40000 [..............................] - ETA: 10:25 - loss: 2.3304 - accuracy: 0.1406WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.105095). Check your callbacks.\n",
      "40000/40000 [==============================] - 4s 112us/sample - loss: 1.7400 - accuracy: 0.3830 - val_loss: 1.4680 - val_accuracy: 0.4792\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 2s 52us/sample - loss: 1.3935 - accuracy: 0.5099 - val_loss: 1.2887 - val_accuracy: 0.5568\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 1.2619 - accuracy: 0.5579 - val_loss: 1.2313 - val_accuracy: 0.5753\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 2s 52us/sample - loss: 1.1669 - accuracy: 0.5918 - val_loss: 1.3181 - val_accuracy: 0.5351\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 2s 54us/sample - loss: 1.0887 - accuracy: 0.6184 - val_loss: 1.0951 - val_accuracy: 0.6229\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 2s 53us/sample - loss: 1.0238 - accuracy: 0.6420 - val_loss: 1.1687 - val_accuracy: 0.5977\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 2s 54us/sample - loss: 0.9718 - accuracy: 0.6585 - val_loss: 1.0828 - val_accuracy: 0.6235\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.9230 - accuracy: 0.6754 - val_loss: 1.0509 - val_accuracy: 0.6388\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 2s 54us/sample - loss: 0.8774 - accuracy: 0.6940 - val_loss: 1.0633 - val_accuracy: 0.6487\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.8297 - accuracy: 0.7107 - val_loss: 1.1736 - val_accuracy: 0.6211\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 2s 50us/sample - loss: 0.7945 - accuracy: 0.7246 - val_loss: 1.0408 - val_accuracy: 0.6560\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.7602 - accuracy: 0.7372 - val_loss: 1.0230 - val_accuracy: 0.6645\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.7240 - accuracy: 0.7495 - val_loss: 0.9799 - val_accuracy: 0.6790\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.6949 - accuracy: 0.7589 - val_loss: 1.0538 - val_accuracy: 0.6686\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 2s 50us/sample - loss: 0.6727 - accuracy: 0.7681 - val_loss: 1.0622 - val_accuracy: 0.6640\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.6477 - accuracy: 0.7757 - val_loss: 1.0102 - val_accuracy: 0.6806\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 2s 50us/sample - loss: 0.6128 - accuracy: 0.7900 - val_loss: 1.0866 - val_accuracy: 0.6626\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.5970 - accuracy: 0.7950 - val_loss: 1.0385 - val_accuracy: 0.6827\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.5745 - accuracy: 0.8040 - val_loss: 1.0486 - val_accuracy: 0.6797\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 2s 50us/sample - loss: 0.5621 - accuracy: 0.8065 - val_loss: 1.1392 - val_accuracy: 0.6654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x186a54dbc88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=build((IMG_ROWS, IMG_COLS, IMG_CHANNELS), CLASSES)\n",
    "model.summary()\n",
    "\n",
    "# use TensorBoard\n",
    "log_dir = 'D:\\\\project_minor\\\\python\\\\golbin2\\\\DeepLearningWithTensorflowAndKeras\\\\logs'\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          validation_split=VALIDATION_SPLIT, verbose=VERBOSE, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/sample - loss: 1.1414 - accuracy: 0.6585\n",
      "\n",
      "Test score: 1.141379397201538\n",
      "Test accuracy: 0.6585\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improbing the CIFAR-10 performance with a deeper network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the performance is to define a deeper network with multiple convolutional operations.  \n",
    "1st module: (CONV+CONV+MaxPool+DropOut)  \n",
    "2nd module: (CONV+CONV+MaxPool+DropOut)  \n",
    "3rd module: (CONV+CONV+MaxPool+DropOut)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BatchNormalization is another form of regularization and one of the most effective improvements proposed during the last few years.  \n",
    "BatchNormalization enables us to accelerate training, in some cases by havling the training epochs, and it offers some regularization.  \n",
    "During training, weights in early layers naturally change and therefore the inputs of later layers can significantly change. In other words, each layer must continuously re-adjust its weights to the different distribution for every batch. This may slow down the model's training greatly. The key idea is to make layer inputs more similar in distribution, batch after batch and epoch after epoch.  \n",
    "Another issue is that the sigmoid activation function works very well close to zero, but tends to \"get stuck\" when values get sufficiently far away from zero. If, occasionally, neuron outputs fluctuate far away from the sigmoid zero, then said neuron becomes unable to update its own weights.  \n",
    "The other key idea is therefore to transform the layer outputs into a Gaussian distribution unit close to zero. In this way, layers will have significantly less variation from batch to batch. Mathematically, the formula is very simple. The activation input $x$ is centered around zero by subtracting the batch mean $\\mu$ from it. Then, the result is divided by $\\sigma + \\epsilon$ to prevent division by zero. Then, we use a linear transformation $y = \\lambda x + \\beta$ to make sure that the normalizing effect is applied during training.  \n",
    "In this way, $\\lambda$ and $\\beta$ are parameters that got optimized during the training phase in a similar way to any other layer. BatchNormalization has been proven as a very effective way to increase both the speed of training and accuracy, because it helps to prevent activations becoming either too small and vanishing or too big and exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "EPOCHS=50\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the convnet\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # 1st block\n",
    "    model.add(layers.Convolution2D(32, (3,3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # 2nd block\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # 3rd block\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    # dense\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # normalize\n",
    "    mean = np.mean(x_train, axis=(0,1,2,3))\n",
    "    std = np.std(x_train, axis=(0,1,2,3))\n",
    "    x_train = (x_train-mean)/(std+1e-7)\n",
    "    x_test = (x_test-mean)/(std+1e-7)\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train, x_test, y_test) = load_data()\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 11s 211us/sample - loss: 1.7387 - accuracy: 0.4858 - val_loss: 1.4445 - val_accuracy: 0.5786\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 6s 128us/sample - loss: 1.1281 - accuracy: 0.6437 - val_loss: 1.3431 - val_accuracy: 0.6194\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.8686 - accuracy: 0.7096 - val_loss: 0.7684 - val_accuracy: 0.7382\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.7509 - accuracy: 0.7488 - val_loss: 0.7502 - val_accuracy: 0.7458\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 6s 130us/sample - loss: 0.6645 - accuracy: 0.7759 - val_loss: 0.6236 - val_accuracy: 0.7837\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 7s 132us/sample - loss: 0.6032 - accuracy: 0.7961 - val_loss: 0.6454 - val_accuracy: 0.7891\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 0.5614 - accuracy: 0.8080 - val_loss: 0.5527 - val_accuracy: 0.8142\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 6s 130us/sample - loss: 0.5186 - accuracy: 0.8223 - val_loss: 0.5826 - val_accuracy: 0.8018\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.4895 - accuracy: 0.8332 - val_loss: 0.5829 - val_accuracy: 0.8045\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 7s 135us/sample - loss: 0.4590 - accuracy: 0.8423 - val_loss: 0.5249 - val_accuracy: 0.8252\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 7s 133us/sample - loss: 0.4376 - accuracy: 0.8484 - val_loss: 0.5613 - val_accuracy: 0.8180\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.4105 - accuracy: 0.8597 - val_loss: 0.4971 - val_accuracy: 0.8310\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 7s 137us/sample - loss: 0.3961 - accuracy: 0.8646 - val_loss: 0.5556 - val_accuracy: 0.8157\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.3781 - accuracy: 0.8697 - val_loss: 0.5196 - val_accuracy: 0.8298\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.3629 - accuracy: 0.8733 - val_loss: 0.4836 - val_accuracy: 0.8387\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.3486 - accuracy: 0.8780 - val_loss: 0.5189 - val_accuracy: 0.8344\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.3322 - accuracy: 0.8844 - val_loss: 0.4915 - val_accuracy: 0.8366\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 7s 132us/sample - loss: 0.3195 - accuracy: 0.8872 - val_loss: 0.5392 - val_accuracy: 0.8290\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 7s 130us/sample - loss: 0.3091 - accuracy: 0.8916 - val_loss: 0.5037 - val_accuracy: 0.8389\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 7s 133us/sample - loss: 0.3015 - accuracy: 0.8942 - val_loss: 0.4847 - val_accuracy: 0.8434\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 7s 135us/sample - loss: 0.2919 - accuracy: 0.8972 - val_loss: 0.4811 - val_accuracy: 0.8456\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 7s 135us/sample - loss: 0.2792 - accuracy: 0.9018 - val_loss: 0.4956 - val_accuracy: 0.8422\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 7s 134us/sample - loss: 0.2719 - accuracy: 0.9046 - val_loss: 0.4802 - val_accuracy: 0.8444\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 7s 136us/sample - loss: 0.2658 - accuracy: 0.9052 - val_loss: 0.4684 - val_accuracy: 0.8555\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 7s 136us/sample - loss: 0.2597 - accuracy: 0.9075 - val_loss: 0.5273 - val_accuracy: 0.8424\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 7s 136us/sample - loss: 0.2495 - accuracy: 0.9106 - val_loss: 0.5080 - val_accuracy: 0.8425\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 0.2411 - accuracy: 0.9145 - val_loss: 0.5124 - val_accuracy: 0.8431\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 7s 135us/sample - loss: 0.2400 - accuracy: 0.9148 - val_loss: 0.4886 - val_accuracy: 0.8516\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 6s 130us/sample - loss: 0.2316 - accuracy: 0.9177 - val_loss: 0.4747 - val_accuracy: 0.8535\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 7s 130us/sample - loss: 0.2279 - accuracy: 0.9183 - val_loss: 0.5013 - val_accuracy: 0.8458\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 7s 134us/sample - loss: 0.2230 - accuracy: 0.9206 - val_loss: 0.4871 - val_accuracy: 0.8559\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 0.2159 - accuracy: 0.9218 - val_loss: 0.4797 - val_accuracy: 0.8512\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 7s 135us/sample - loss: 0.2131 - accuracy: 0.9238 - val_loss: 0.5189 - val_accuracy: 0.8481\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 7s 136us/sample - loss: 0.2040 - accuracy: 0.9255 - val_loss: 0.4847 - val_accuracy: 0.8555\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 7s 133us/sample - loss: 0.2077 - accuracy: 0.9244 - val_loss: 0.4986 - val_accuracy: 0.8532\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 7s 130us/sample - loss: 0.2004 - accuracy: 0.9285 - val_loss: 0.5048 - val_accuracy: 0.8544\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 6s 130us/sample - loss: 0.1976 - accuracy: 0.9299 - val_loss: 0.4973 - val_accuracy: 0.8516\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 7s 132us/sample - loss: 0.1940 - accuracy: 0.9297 - val_loss: 0.4868 - val_accuracy: 0.8595\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 7s 133us/sample - loss: 0.1904 - accuracy: 0.9314 - val_loss: 0.5106 - val_accuracy: 0.8561\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 6s 128us/sample - loss: 0.1840 - accuracy: 0.9343 - val_loss: 0.5284 - val_accuracy: 0.8529\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 6s 130us/sample - loss: 0.1808 - accuracy: 0.9350 - val_loss: 0.5044 - val_accuracy: 0.8542\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 7s 130us/sample - loss: 0.1806 - accuracy: 0.9345 - val_loss: 0.4838 - val_accuracy: 0.8588\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.1777 - accuracy: 0.9354 - val_loss: 0.4803 - val_accuracy: 0.8615\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.1785 - accuracy: 0.9365 - val_loss: 0.5023 - val_accuracy: 0.8558\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.1770 - accuracy: 0.9359 - val_loss: 0.5075 - val_accuracy: 0.8606\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.1715 - accuracy: 0.9377 - val_loss: 0.5040 - val_accuracy: 0.8553\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 6s 129us/sample - loss: 0.1694 - accuracy: 0.9387 - val_loss: 0.5515 - val_accuracy: 0.8531\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 6s 128us/sample - loss: 0.1687 - accuracy: 0.9395 - val_loss: 0.4849 - val_accuracy: 0.8617\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.1621 - accuracy: 0.9419 - val_loss: 0.5147 - val_accuracy: 0.8571\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 6s 128us/sample - loss: 0.1607 - accuracy: 0.9420 - val_loss: 0.5155 - val_accuracy: 0.8560\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8745556a5b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTest score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "batch_size = 64\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.5155 - accuracy: 0.8560\n",
      "\n",
      "Test score: 0.5155008586883545\n",
      "Test accuracy: 0.856\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the CIFAR-10 performance with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to improve the performance is to generate more images for our training. The idea here is that we can take the standard CIFAR training set and augment this set with multiple types of transformation, including rotation, rescaling, horizontal or vertical flip, zooming, channel shift, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# image augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`rotation-ragne`** is a value in degrees (0-180) for randomly rotating pictures; **`width_shift`** and **`height_shift`** are ranges for randomly translating pictures vertically or horizontally. **`zoom_range`** is for randomly zooming pictures; **`horizontal_flip`** is for randomly flipping half of the images horizontally; and **`fill_mode`** is the strategy used for filling in new pixels that can appear after a rotation or a shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this intuition directly for training. For efficiency, the generator runs in parallel to the model. This allows image augmentation on a CPU while training in parallel on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ce6151cd32e1>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 782 steps, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 22s 29ms/step - loss: 2.1419 - accuracy: 0.3515 - val_loss: 1.5328 - val_accuracy: 0.4851\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 1.5548 - accuracy: 0.4842 - val_loss: 1.4076 - val_accuracy: 0.5558\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 1.2995 - accuracy: 0.5552 - val_loss: 1.0591 - val_accuracy: 0.6457\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 1.1431 - accuracy: 0.6007 - val_loss: 1.0455 - val_accuracy: 0.6492\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 1.0395 - accuracy: 0.6357 - val_loss: 0.9141 - val_accuracy: 0.6950\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.9690 - accuracy: 0.6605 - val_loss: 0.8494 - val_accuracy: 0.7103\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.9157 - accuracy: 0.6799 - val_loss: 0.7707 - val_accuracy: 0.7431\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.8748 - accuracy: 0.6954 - val_loss: 0.7396 - val_accuracy: 0.7536\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.8466 - accuracy: 0.7082 - val_loss: 0.8334 - val_accuracy: 0.7325\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.8128 - accuracy: 0.7175 - val_loss: 0.7449 - val_accuracy: 0.7599\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.7926 - accuracy: 0.7248 - val_loss: 0.7247 - val_accuracy: 0.7599\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.7683 - accuracy: 0.7331 - val_loss: 0.6555 - val_accuracy: 0.7824\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.7495 - accuracy: 0.7387 - val_loss: 0.6858 - val_accuracy: 0.7766\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.7287 - accuracy: 0.7487 - val_loss: 0.7118 - val_accuracy: 0.7721\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.7196 - accuracy: 0.7489 - val_loss: 0.6321 - val_accuracy: 0.7932\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.7027 - accuracy: 0.7558 - val_loss: 0.6288 - val_accuracy: 0.7946\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6921 - accuracy: 0.7600 - val_loss: 0.6163 - val_accuracy: 0.7944\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6859 - accuracy: 0.7636 - val_loss: 0.6013 - val_accuracy: 0.8081\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6707 - accuracy: 0.7678 - val_loss: 0.5953 - val_accuracy: 0.8038\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6643 - accuracy: 0.7727 - val_loss: 0.5803 - val_accuracy: 0.8099\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6577 - accuracy: 0.7730 - val_loss: 0.6136 - val_accuracy: 0.8009\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6441 - accuracy: 0.7795 - val_loss: 0.5950 - val_accuracy: 0.8073\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6365 - accuracy: 0.7803 - val_loss: 0.6334 - val_accuracy: 0.7882\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6302 - accuracy: 0.7829 - val_loss: 0.6111 - val_accuracy: 0.8056\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6289 - accuracy: 0.7838 - val_loss: 0.6413 - val_accuracy: 0.7988\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6191 - accuracy: 0.7863 - val_loss: 0.6150 - val_accuracy: 0.7991\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6101 - accuracy: 0.7903 - val_loss: 0.5256 - val_accuracy: 0.8269\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6070 - accuracy: 0.7900 - val_loss: 0.5714 - val_accuracy: 0.8175\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6013 - accuracy: 0.7912 - val_loss: 0.5495 - val_accuracy: 0.8179\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5995 - accuracy: 0.7922 - val_loss: 0.5294 - val_accuracy: 0.8267\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5964 - accuracy: 0.7945 - val_loss: 0.5009 - val_accuracy: 0.8330\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5835 - accuracy: 0.7984 - val_loss: 0.5288 - val_accuracy: 0.8238\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5846 - accuracy: 0.7975 - val_loss: 0.6422 - val_accuracy: 0.8053\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5797 - accuracy: 0.7994 - val_loss: 0.5849 - val_accuracy: 0.8124\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5789 - accuracy: 0.7996 - val_loss: 0.4968 - val_accuracy: 0.8393\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5770 - accuracy: 0.8013 - val_loss: 0.6005 - val_accuracy: 0.8099\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5708 - accuracy: 0.8021 - val_loss: 0.5324 - val_accuracy: 0.8279\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5664 - accuracy: 0.8035 - val_loss: 0.5425 - val_accuracy: 0.8250\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5665 - accuracy: 0.8059 - val_loss: 0.5654 - val_accuracy: 0.8182\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5579 - accuracy: 0.8076 - val_loss: 0.5028 - val_accuracy: 0.8331\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5603 - accuracy: 0.8084 - val_loss: 0.5664 - val_accuracy: 0.8219\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5522 - accuracy: 0.8102 - val_loss: 0.5619 - val_accuracy: 0.8191\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5522 - accuracy: 0.8091 - val_loss: 0.5299 - val_accuracy: 0.8268\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5523 - accuracy: 0.8086 - val_loss: 0.4899 - val_accuracy: 0.8420\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5428 - accuracy: 0.8123 - val_loss: 0.5444 - val_accuracy: 0.8267\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.5407 - accuracy: 0.8122 - val_loss: 0.4751 - val_accuracy: 0.8415\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5394 - accuracy: 0.8134 - val_loss: 0.5756 - val_accuracy: 0.8219\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5351 - accuracy: 0.8141 - val_loss: 0.4742 - val_accuracy: 0.8467\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5374 - accuracy: 0.8132 - val_loss: 0.4777 - val_accuracy: 0.8430\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.5296 - accuracy: 0.8174 - val_loss: 0.5647 - val_accuracy: 0.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e8df01a88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=EPOCHS, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "model_json = model.to_json()\n",
    "with open('./models/cifar10_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('./models/cifar10_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.5647 - accuracy: 0.8252\n",
      "\n",
      "Test result: 82.520 loss: 0.565\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100, scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of state-of-the-art results for CIFAR-10 is available online (http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html). As of April 2019, the best result has an accuracy of 96.53%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose the we want to use the deep learning model we just trained for CIFAR-10 for a bulk evaluation of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from imageio import imread\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_architecture = './models/cifar10_architecture.json'\n",
    "model_weights = './models/cifar10_weights.h5'\n",
    "model = model_from_json(open(model_architecture).read())\n",
    "model.load_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape: (2, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "dir = 'D:/project_minor/python/golbin2/DeepLearningWithTensorflowAndKeras/sources/Chapter 4/'\n",
    "img_names = [dir+'cat-standing.jpg', dir+'dog.jpg']\n",
    "imgs = [resize(imread(img_name), (32, 32)).astype('float32') for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n",
    "print('imgs.shape:', imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "optim = SGD()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = model.predict_classes(imgs)\n",
    "print('predictions:', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very deep convolutional networks for large-scale image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the ImageNet ILSVRC-2012 ( http://image-net.org/challenges/LSVRC/2012/ ) dataset, which includes images of 1,000 classes, and is split into three sets: training (1.3 million images), validation (50,000 images), and testing (100,000 images). Each image is (224224) on 3 channels. The model achieves 7.5% top-5 error on ILSVRC-2012-val, 7.4% top-5 error on ILSVRC-2012-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a VGG16 network\n",
    "def VGG_16(weights_path=None):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.ZeroPadding2D((1,1), input_shape=(244,244,3)))\n",
    "    model.add(layers.Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(128, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(128, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(256, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(256, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(256, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.ZeroPadding2D((1,1)))\n",
    "    model.add(layers.Convolution2D(512, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    # top layer of the VGG net\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1000, activation='softmax'))\n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a VGG16. Next, we are going to utilize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing cats with a VGG16 Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us test the image of a cat.  \n",
    "Note that we are going to use predefined weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = dir = 'D:/project_minor/python/golbin2/DeepLearningWithTensorflowAndKeras/sources/Chapter 4/'\n",
    "im = cv2.resize(cv2.imread(dir + 'cat.jpg'), (244, 244)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im = im.transpose((2, 0, 1))\n",
    "im = np.expand_dims(im, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pretrained model\n",
    "path_file = 'D:/project_minor/python/golbin2/DeepLearningWithTensorflowAndKeras/sources/Chapter 4/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_16(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_22 (ZeroPaddi (None, 246, 246, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 244, 244, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPaddi (None, 246, 246, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 244, 244, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 122, 122, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 122, 122, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, 124, 124, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 122, 122, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 61, 61, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPaddi (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 61, 61, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_27 (ZeroPaddi (None, 63, 63, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 61, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_28 (ZeroPaddi (None, 63, 63, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 61, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_29 (ZeroPaddi (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 30, 30, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_30 (ZeroPaddi (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 30, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_31 (ZeroPaddi (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 30, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_32 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_33 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_34 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "out = model.predict(im)\n",
    "print(np.argmax(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the code is executed, the class 285 is returned, which corresponds to \"Egyptiam cat\"\n",
    "(https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing tf.keras built-in VGG16 Net module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.keras`** applications are prebuilt and pretrained deep learning models. Weights are downloaded automatically when instantiating a model and stored at **`~/.keras/models/~`**. Using built-in code is very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 167s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[159., 148., 146.],\n",
       "         [170., 156., 149.],\n",
       "         [169., 150., 144.],\n",
       "         ...,\n",
       "         [159., 139., 122.],\n",
       "         [155., 136., 119.],\n",
       "         [160., 140., 123.]],\n",
       "\n",
       "        [[163., 151., 147.],\n",
       "         [193., 178., 171.],\n",
       "         [190., 171., 164.],\n",
       "         ...,\n",
       "         [155., 132., 116.],\n",
       "         [158., 133., 117.],\n",
       "         [158., 134., 118.]],\n",
       "\n",
       "        [[166., 153., 144.],\n",
       "         [163., 148., 140.],\n",
       "         [174., 157., 148.],\n",
       "         ...,\n",
       "         [158., 135., 117.],\n",
       "         [153., 127., 111.],\n",
       "         [159., 135., 119.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 91.,  99., 106.],\n",
       "         [ 48.,  60.,  63.],\n",
       "         [ 33.,  32.,  38.],\n",
       "         ...,\n",
       "         [132., 123., 132.],\n",
       "         [169., 168., 172.],\n",
       "         [158., 153., 158.]],\n",
       "\n",
       "        [[  8.,  10.,  18.],\n",
       "         [ 18.,  15.,  23.],\n",
       "         [  0.,   2.,   7.],\n",
       "         ...,\n",
       "         [161., 157., 162.],\n",
       "         [168., 158., 164.],\n",
       "         [157., 146., 152.]],\n",
       "\n",
       "        [[105., 105., 120.],\n",
       "         [ 42.,  44.,  57.],\n",
       "         [115., 121., 131.],\n",
       "         ...,\n",
       "         [134., 130., 135.],\n",
       "         [154., 148., 153.],\n",
       "         [168., 162., 167.]]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize into VGG16 trained images' format\n",
    "dir = 'D:/project_minor/python/golbin2/DeepLearningWithTensorflowAndKeras/sources/Chapter 4/'\n",
    "im = cv2.resize(cv2.imread(dir + 'steam-locomotive.jpg'), (224, 224))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict\n",
    "out = model.predict(im)\n",
    "index = np.argmax(out)\n",
    "print(index)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "# this should print 820 for steaming train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other classes have very weak support, as shown in the preceding figure  \n",
    "VGG16 is only one of the modules that is prebuilt in **`tf.keras`**. A full list of pretrained models is available online (https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycling prebuilt deep learning models for extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very simple idea is to use VGG16, and more generally DCNN, for feature extraction. This code implements the idea by extracting features from a specific layer. Note that we need to switch to the functional API since the sequential model only accepts layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.training.Model object at 0x000002514EF67708>\n"
     ]
    }
   ],
   "source": [
    "# prebuild model with pre-trained weights on imagenet\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3 [(None, 224, 224, 3)]\n",
      "1 block1_conv1 (None, 224, 224, 64)\n",
      "2 block1_conv2 (None, 224, 224, 64)\n",
      "3 block1_pool (None, 112, 112, 64)\n",
      "4 block2_conv1 (None, 112, 112, 128)\n",
      "5 block2_conv2 (None, 112, 112, 128)\n",
      "6 block2_pool (None, 56, 56, 128)\n",
      "7 block3_conv1 (None, 56, 56, 256)\n",
      "8 block3_conv2 (None, 56, 56, 256)\n",
      "9 block3_conv3 (None, 56, 56, 256)\n",
      "10 block3_pool (None, 28, 28, 256)\n",
      "11 block4_conv1 (None, 28, 28, 512)\n",
      "12 block4_conv2 (None, 28, 28, 512)\n",
      "13 block4_conv3 (None, 28, 28, 512)\n",
      "14 block4_pool (None, 14, 14, 512)\n",
      "15 block5_conv1 (None, 14, 14, 512)\n",
      "16 block5_conv2 (None, 14, 14, 512)\n",
      "17 block5_conv3 (None, 14, 14, 512)\n",
      "18 block5_pool (None, 7, 7, 512)\n",
      "19 flatten (None, 25088)\n",
      "20 fc1 (None, 4096)\n",
      "21 fc2 (None, 4096)\n",
      "22 predictions (None, 1000)\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from block4_pool block\n",
    "model = models.Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "dir = 'D:/project_minor/python/golbin2/DeepLearningWithTensorflowAndKeras/sources/Chapter 4/'\n",
    "img_path = dir + 'cat.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x24FE6B32E88>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  0.         0.        39.127655 ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.       261.40973\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.       376.41425\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       175.46494\n",
      "      0.      ]\n",
      "   [  0.         0.        32.00825  ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.         0.        76.28149  ...   0.         0.\n",
      "      0.      ]]\n",
      "\n",
      "  [[  0.         0.         0.       ...   0.        50.92023\n",
      "      0.      ]\n",
      "   [  0.         0.        44.452904 ...   0.       123.95452\n",
      "      0.      ]\n",
      "   [  0.         0.        27.892153 ...   0.       573.44165\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       683.4282\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.        18.34751\n",
      "      0.      ]\n",
      "   [  0.         0.        60.561913 ...   0.         0.\n",
      "      0.      ]]\n",
      "\n",
      "  [[  0.         0.        27.617643 ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.       372.48425    0.       ...  29.134195   0.\n",
      "      0.      ]\n",
      "   [  0.        34.772655 105.49562  ...   0.         0.\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       823.46704\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.        95.23555\n",
      "      0.      ]\n",
      "   [  0.         0.        20.0432   ...   0.         0.\n",
      "      0.      ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.         0.         0.       ...   0.        58.513058\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       376.7513\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.         0.         9.00437  ...   0.         0.\n",
      "      0.      ]]\n",
      "\n",
      "  [[  0.         0.         0.       ...   0.       112.54091\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.        48.29862\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       340.15564\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]]\n",
      "\n",
      "  [[  0.         0.        33.47351  ...   0.        89.46333\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.       169.87428\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.         0.\n",
      "      0.      ]\n",
      "   ...\n",
      "   [  0.         0.         0.       ...   0.       142.92583\n",
      "      0.      ]\n",
      "   [  0.         0.         0.       ...   0.        14.979053\n",
      "      0.      ]\n",
      "   [  0.         0.        51.72882  ...   0.         0.\n",
      "      0.      ]]]]\n"
     ]
    }
   ],
   "source": [
    "# get the features from this block\n",
    "features = model.predict(x)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why we wnat to extract the features from an intermediate layer in a DCNN. The reasoning is that as the network learns to classify images into categories, each layer learns to identify the features that are necessary to perform the final classification. Lower layers identify lower-order features such as color and edges, and higher layers compose these lower-order features into higher-order features such as shapes or objects. Hence, the intermediate layer has the capability to extract important features from an image, and these features are more likely to help in different kinds of classification.  \n",
    "This has multiple advantages. Fist, we can rely on publicly available large-scale training and transfer this learning to novel domains. Second, we can save time for expensive large training. Third, we can provide reasonable solutions even when we don't have a large number of training examples for our domain. We also get a good starting network shape for the task at hand, instead of guessing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
