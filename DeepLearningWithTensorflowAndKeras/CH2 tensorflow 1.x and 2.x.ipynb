{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensorflow 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Const_2:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t_1 = tf.constant(4)\n",
    "print(t_1)\n",
    "t_2 = tf.constant([4,3,2])\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create a tensor with all elements are zero\n",
    "z = tf.zeros([3,3], tf.float32)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_like:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"ones_like:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# create a zero matrix of same shape as t_2\n",
    "print(tf.zeros_like(t_2))\n",
    "# create a ones matrix of same shape as t_2\n",
    "print(tf.ones_like(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones([3,2], tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "s = tf.linspace(1.0, 5.0, 5)\n",
    "print(s)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(s)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "r = tf.range(10)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(r)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2534746  5.3799095  1.9527606]\n",
      " [-1.5376031  1.2588985  2.8478067]]\n"
     ]
    }
   ],
   "source": [
    "# normal distribution\n",
    "t_random = tf.random_normal([2,3], mean=2.0, stddev=4, seed=12)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(t_random)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43663135  0.84497744 -0.01180986]\n",
      " [-0.8844008  -1.938745    0.5632738 ]]\n"
     ]
    }
   ],
   "source": [
    "# truncated normal distribution\n",
    "t_random = tf.truncated_normal([2,3], seed=12)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(t_random)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.54461   3.6963658 2.7051091]\n",
      " [2.0085006 3.8445983 3.5426888]]\n"
     ]
    }
   ],
   "source": [
    "t_random = tf.random_uniform([2,3], maxval=4, seed=12)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(t_random)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.54461   3.6963658 2.7051091]\n",
      " [2.0085006 3.8445983 3.5426888]]\n"
     ]
    }
   ],
   "source": [
    "t_rc = tf.random_crop(t_random, [2,3], seed=12)\n",
    "with tf.Session() as sess:\n",
    "    outs = sess.run(t_rc)\n",
    "    print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RandomShuffle:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly shuffle a tensor\n",
    "tf.random_shuffle(t_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 세션의 모든 랜덤 생성 텐서에 동일한 시드를 적용\n",
    "tf.set_random_seed(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([100, 100], stddev=2))\n",
    "bias = tf.Variable(tf.zeros([100]), name = 'biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 방법을 지정해둔 Variable을 명시적으로 초기화한다\n",
    "initial_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver class로 Variable을 저장할 수 있다\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(x):\n",
    "    return 3 * x + 2\n",
    "\n",
    "@tf.function\n",
    "def simple_nn(x):\n",
    "    return tf.nn.relu(linear_layer(x))\n",
    "\n",
    "def simple_function(x):\n",
    "    return 3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.eager.def_function.Function object at 0x0000025A09308CC8>\n",
      "<function simple_function at 0x0000025A092CC438>\n"
     ]
    }
   ],
   "source": [
    "# simple_nn is a special handler for interacting with TensorFlow internals\n",
    "# while simple_function is a normal Python handler\n",
    "print(simple_nn)\n",
    "print(simple_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that with tf.function you need to annotate only one main function, \n",
    "# so all other functions called from there will be automatically transformed into\n",
    "# an optimized computational graph\n",
    "# tf.function marking code for Just In Time(JUT) compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__simple_nn(x):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('simple_nn', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "    do_return = True\n",
      "    retval_ = fscope.mark_return_value(ag__.converted_call(tf.nn.relu, (ag__.converted_call(linear_layer, (x,), None, fscope),), None, fscope))\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# internal look at the auto-generated code\n",
    "print(tf.autograph.to_code(simple_nn.python_function, experimental_optional_features=None))\n",
    "# automatically generated code will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_time: 0.04774670000006154\n",
      "auto_graph_time: 0.03041829999995116\n"
     ]
    }
   ],
   "source": [
    "# let's check speed between code annotated with the tf.function() decorator\n",
    "# and the same code with no annotation with using a layer LSTMCell()\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "cell = tf.keras.layers.LSTMCell(100)\n",
    "\n",
    "@tf.function\n",
    "def fn(input, state):\n",
    "    return cell(input, state)\n",
    "\n",
    "input = tf.zeros([100, 100])\n",
    "state = [tf.zeros([100, 100])] * 2\n",
    "\n",
    "#warmup\n",
    "cell(input, state)\n",
    "fn(input, state)\n",
    "\n",
    "graph_time = timeit.timeit(lambda: cell(input, state), number=100)\n",
    "auto_graph_time = timeit.timeit(lambda: fn(input, state), number=100)\n",
    "\n",
    "print('graph_time:', graph_time)\n",
    "print('auto_graph_time:', auto_graph_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, you can decorate Python functions and methods with tf.function, which converts them to the equivalent of a static graph, with all the optimization that comes with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras APIs - three programming models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API is useful when you want to build a model with more complex (non-linear) topologies, including multiple inputs, multiple outputs, residual\n",
    "connections with non-sequential flows, and shared and resuable layers. Each layer is callable (with a tensor in input), and each layer returns a tensor as an output. Let's look at an example where we have two separate inputs, two separate logistic regressions as outputs, and one shared module in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_model():\n",
    "    \n",
    "    # variable-length sequence of integers\n",
    "    text_input_a = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # variable-length sequence of integers\n",
    "    text_input_b = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Embedding for 1000 unique words mapped to 128-dimensional vectors\n",
    "    shared_embedding = tf.keras.layers.Embedding(1000, 128)\n",
    "    \n",
    "    # we resue the same layer to encode both inputs\n",
    "    encoded_input_a = shared_embedding(text_input_a)\n",
    "    encoded_input_b = shared_embedding(text_input_b)\n",
    "    \n",
    "    # two logistic predictions at the end\n",
    "    # create a layer, then pass it an input\n",
    "    prediction_a = tf.keras.layers.Dense(1, activation='sigmoid', name='prediction_a')(encoded_input_a)\n",
    "    prediction_b = tf.keras.layers.Dense(1, activation='sigmoid', name='prediction_b')(encoded_input_b)\n",
    "    \n",
    "    # this model has 2 inputs, and 2 outputs\n",
    "    # in the middle we have a shared model\n",
    "    model = tf.keras.Model(inputs=[text_input_a, text_input_b], outputs=[prediction_a, prediction_b])\n",
    "    \n",
    "    tf.keras.utils.plot_model(model, to_file='shared_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model subclassing offers the highest flexbility and it is generally used when you need to define your own layer. In other words, it is useful when you are in the business of building your own special lego brick instead of composing more standard and well-known bricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a custom layer, we can subclass tf.keras.layers.Layer and implement the following methods:\n",
    "<ul>\n",
    "    <li>__init__ : Optionally used to define all the sublayers to be used by this layer. this is constructor where you can declare your model.\n",
    "    <li>build : Used to create the weights of the layer. You can add weights with add_weight()\n",
    "    <li>call : Used to define the forward pass. This is where your layer is called and chained in functional style.\n",
    "    <li>Optionally, a layer can be serialized by using get_config() and deserialized using from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see an exmple of a custom layer that simply multiples and input by a matrix named kernel\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # create a trainable weight variable for this layer\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Do the multiplication and return\n",
    "        return tf.matmul(inputs, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the MyLayer() custom brick is defined, it can be composed just like any other brick\n",
    "model = tf.keras.Sequential([MyLayer(20), tf.keras.layers.Activation('softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are objects passed to a model to extend or modify behaviors during training\n",
    "<ul>\n",
    "    <li>tf.keras.callbacks.ModelCheckPoint : This feature is used to save checkpoints of your model at regular intervals and recover in case of problems.\n",
    "    <li>tf.keras.callbacks.LearningRateScheduler : This feature is used to dynamically change the learning rate duringg optimization\n",
    "    <li>tf.keras.callbacks.EarlyStopping : This feature is sued to interrupt training when validation performance has stopped improbing after a while\n",
    "    <li>tf.keras.callbacks.TensorBoard : This feature is used to monitor the model's behavior using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Write TensorBoard logs to './logs' directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=256, epochs=100, callbacks=callbacks, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to a Tensorflow Checkpoint file\n",
    "model.save_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save in Keras's format, which is portable across multiple backends\n",
    "# Save weights to a HDF5 file\n",
    "model.save_weights('my_model.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model's state\n",
    "model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model can be serialized in JSON with:\n",
    "json_string = tf.keras.models.model_from_json(json_string) # rstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you prefer, a model can be serialized in YAML with:\n",
    "yaml_string = model.to_yaml() # save\n",
    "model = tf.keras.model.model_from_yaml(yaml_string) # restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save a model together with its weights and the optimization parameters,\n",
    "model.save('my_model.h5') # save\n",
    "model = tf.keras.models.load_model('my_model.h5') # restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from tf.data.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'aflw2k3d', 'amazon_us_reviews', 'bair_robot_pushing_small', 'bigearthnet', 'binarized_mnist', 'binary_alpha_digits', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'chexpert', 'cifar10', 'cifar100', 'cifar10_corrupted', 'clevr', 'cnn_dailymail', 'coco', 'coco2014', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'curated_breast_imaging_ddsm', 'cycle_gan', 'deep_weeds', 'definite_pronoun_resolution', 'diabetic_retinopathy_detection', 'downsampled_imagenet', 'dsprites', 'dtd', 'dummy_dataset_shared_generator', 'dummy_mnist', 'emnist', 'eurosat', 'fashion_mnist', 'flores', 'food101', 'gap', 'glue', 'groove', 'higgs', 'horses_or_humans', 'image_label_folder', 'imagenet2012', 'imagenet2012_corrupted', 'imdb_reviews', 'iris', 'kitti', 'kmnist', 'lfw', 'lm1b', 'lsun', 'mnist', 'mnist_corrupted', 'moving_mnist', 'multi_nli', 'nsynth', 'omniglot', 'open_images_v4', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'pet_finder', 'quickdraw_bitmap', 'resisc45', 'rock_paper_scissors', 'rock_you', 'scene_parse150', 'shapes3d', 'smallnorb', 'snli', 'so2sat', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tf_flowers', 'titanic', 'trivia_qa', 'uc_merced', 'ucf101', 'visual_domain_decathlon', 'voc2007', 'wikipedia', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'xnli']\n"
     ]
    }
   ],
   "source": [
    "# See all registered datasets\n",
    "builders = tfds.list_builders()\n",
    "print(builders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to C:\\Users\\polas\\tensorflow_datasets\\mnist\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f75d97bbeb4d44a4256e082635a94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ea4874c8db45c6bc366ec15662a857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52a3f6a959b40c8b700f682d6cec4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'storage.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'storage.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'storage.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'storage.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Shuffling...', max=10.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\miniconda3\\envs\\golbin2\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=6000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Shuffling...', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Reading...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Writing...', max=10000.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\polas\\tensorflow_datasets\\mnist\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=1.0.0,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a given dataset by name, along with the DatasetInfo metadata\n",
    "data, info = tfds.load('mnist', with_info=True)\n",
    "train_data, test_data = data['train'], data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=1.0.0,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we get the metainfo for MNIST\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes it is useful to create a dataset from a NumPy array\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "num_items = 100\n",
    "num_list = np.arange(num_items)\n",
    "\n",
    "# create the dataset from numpy array\n",
    "num_list_dataset = tf.data.Dataset.from_tensor_slices(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "print(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "print(num_list_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to C:\\Users\\polas\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857f578037a144f292e5a2f890e3c374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d643f517944f39b1a5c35567583e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also download a dataset, shuffle and batch the data, and take a slice from the generator\n",
    "datasets, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset = datasets['train']\n",
    "# shuffle() is a transformation that randomly shuffles the input dataset\n",
    "# batch() creates batches of tensors\n",
    "train_dataset = train_dataset.batch(5).shuffle(50).take(2)\n",
    "for data in train_dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset is a library for dealing with data in principled way.\n",
    "Operations include:\n",
    "<ol><li>Creation:\n",
    "        <ol><li>from_tensor_slices() : which accepts individual (or multiple) NumPy (or tensors) and supports batches\n",
    "            <li>from_tensors() : which is similar to the above but it does not support batches\n",
    "            <li>from_generator() : which takes input from a generator function</ol><br>\n",
    "    <li>Transformation:\n",
    "        <ol><li>batch() : which sequentially devides the dataset by the specified size\n",
    "            <li>repeat() : which duplicates the data\n",
    "            <li>shuffle() : which randomly shuffles the data\n",
    "            <li>map() : which applies a function to the data\n",
    "            <li>filter() : which applies a function to filter the data</ol><br>\n",
    "    <li>Iterators:\n",
    "        <ol><li>next_batch = iterator.get_next()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset uses TFRecord, a representation of the data (in any format) that can be easily ported across multiple systems and is independent of the particular model used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras or Estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Estimators? Put simply, they are another way to build or to use prebuilt bricks. A longer answer is that they are highly efficient learning models for large-scale production-ready environments, which can be trained on single machines or on distributed nulti-servers, and they can run on CPUs, GPUs, or TPUs without recoding your model. These models include Linear Classifiers, Deep Learning Classifiers, Gradient Boosted Trees, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of an Estimator used for building a classifier with 2 dense hidden layers, each with 10 neurons, and with 3 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature_columns=my_feature_columns is a list of feature columns each describing a single feature your want the model to use. For example, a typical use would be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data\n",
    "(train_x, train_y), (test_x, test_y) = load_data()\n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, tf.feature_column.numeric_column() represents real valued or numerical features (https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency Estimators should be trained using tf.Datasets as input. Here is an example where MNIST is loaded, scaled, shuffled, and batched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "def input_fn(mode):\n",
    "    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else datasets['test'])\n",
    "    \n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    " \n",
    "    return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n",
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "test = input_fn('test')\n",
    "train = input_fn(tf.estimator.ModeKeys.TRAIN)\n",
    "print(test)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the Estimator can be trained and evaluated by using tf.estimator.train_and_evaluate() and passing the input_fn which will iterate on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    classifier,\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=input_fn),\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 2.x added support for \"ragged\" tensors, which are a special type of dense tensor with non-uniformly shaped dimensions. This is particulary useful for dealing with sequences and other data issues where the dimensions can change across batches, such as text sentences and hierarchical data. Note that ragged tensors are more efficient than padding tf.Tensor, since no time or space is wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2, 3], [3, 4], [5, 6, 7, 8]]>\n"
     ]
    }
   ],
   "source": [
    "ragged = tf.ragged.constant([[1,2,3], [3,4], [5,6,7,8]])\n",
    "print(ragged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom training is useful when you want to have finer control over optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf.GradientTape() : This class records operations for automatic differentiation. Let's look at an example where we use the parameter persistent=True (a Boolean controlling whether a persistent gradient tape is created, which means that multiple calls can be made to the gradient() method on this object):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "persistence는 다음과 같이 정의된다.\n",
    "\n",
    "Persistence는 객체의 존재가 시간(즉, 객체는 객체를 생성한 무엇인가가 존재를 끝마치게 한 후에 계속해서 존재함)이나 공간(즉, 객체의 위치는 객체가 생성되었던 주소 공간에서 부터 이동됨)을 초월해서 여기저기 옮겨다니는(through) 객체의 성질(property)이다.\n",
    "\n",
    "이상과 같이 Booch 책에서 persistence라는 의미를 설명한 내용이다. 마지막에 요약되어서 나와있지만, 객체가 시간과 공간을 초월하여 여러 프로그램간을 옮겨다닐 수 있는 성질로 표현할 수 있는데, 딱히 우리말로 한단어로 표현하기가 쉽지 않다. 프로그래머가 가장 이해하기 쉬운 단어는 '객체의 저장' 정도가 되겠지만, 이렇게 번역하기는 또한 persistence가 갖게 되는 다른 의미가 많이 상쇄되는 느낌이다. 왜냐하면 '저장' 이라는 의미가 주로 DB 저장을 뜻하는 경우가 대부분이기 때문일 것이다. 아무튼 'DB 저장' 과는 별도의 개념인 '객체 저장(persistence)' 이라는 개념이 필요할 것 같다.\n",
    "\n",
    "출처: https://homo-ware.tistory.com/4 [人-ware : Forwards Veritas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(256.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant(4.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = g.gradient(z, x) # 256.0 (4*x^3 at x = 4)\n",
    "dy_dx = g.gradient(y, x) # 8.0\n",
    "print(dz_dx)\n",
    "print(dy_dx)\n",
    "del g # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tf.gradient_function() : This returns a function that computes the derivatives of its inpuit function parameter with respect to its arguments.\n",
    "<br><br>\n",
    "3. tf.value_and_gradients_function() : This returns the value from the input function in addition to the list of derivatives of the input function with respect to its arguments\n",
    "<br><br>\n",
    "4. tf.implicit_gradients() : This computes the gradients of the outputs of the input function with regards to all trainable variables these outputs depend on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a skeleton of a custom gradient computation where a model is given as input, and the training steps are computing total_loss = pred_loss + regularization_loss. The decorator @tf.function is used for AutoGraph, and tape.gradient() and apply_gradients() are used to compute and apply the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        regularization_loss = # TBD according to the problem\n",
    "        pred_loss = # TBD according th toe problem\n",
    "        total_loss = pred_loss + regularization_loss\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the training step train_step(inputs, labels) is applied for each epoch, for each input and its associated label in train_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for inputs, labels in train_data:\n",
    "        train_step(inputs, labels)\n",
    "    print(\"Finished epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, put simply, GradientTape() allows us to control and change how the training process is performed internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed training in TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can switch between GPUs, TPUs, and muptiple machines by just changing the tf.distribute.Strategy instance.\n",
    "Strategies can be synchronous, where all workers train over different slices of input data in a form of sync data parallel computaiton, or asynchronous, where updates from the optimizers are not happening in sync. All strategies require that data is loaded in batches the API tf.data.Dataset API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In order to load out data in a way that can be distributed into the GPUs, we simply need a tf.data.Dataset. If we do not have a tf.data.Dataset but we have a normal tensor, then we can easily conver the latter into the former using tf.data.Dataset.from_tensors_slices(). This will take a tensor in memory and return a source dataset, the elements of which are slices of the given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.7064347  0.21554118 0.16618264 ... 0.7284623  0.8475142  0.38597292]\n",
      " [0.02400104 0.5342169  0.5304134  ... 0.9043175  0.45012575 0.2092488 ]\n",
      " [0.2288879  0.23640999 0.9511873  ... 0.5638631  0.557472   0.72549695]\n",
      " ...\n",
      " [0.66274357 0.29937664 0.33537513 ... 0.0792544  0.68229526 0.86084676]\n",
      " [0.20431727 0.847871   0.62121135 ... 0.43766165 0.45112902 0.42994523]\n",
      " [0.9513815  0.9032169  0.87162656 ... 0.4489549  0.8452597  0.16410215]], shape=(1048576, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "N_TRAIN_EXAMPLES = 1024*1024\n",
    "N_FEATURES = 10\n",
    "SIZE_BATCHES = 256\n",
    "\n",
    "# 10 random floats in the half-open interval [0.0, 1.0).\n",
    "x = np.random.random((N_TRAIN_EXAMPLES, N_FEATURES))\n",
    "y = np.random.randint(2, size=(N_TRAIN_EXAMPLES, 1))\n",
    "x = tf.dtypes.cast(x, tf.float32)\n",
    "print(x)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "dataset = dataset.shuffle(buffer_size=N_TRAIN_EXAMPLES).batch(SIZE_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In order to distribute some eomputations to GPUs, we instantiate a distribution = tf.distribute.MirroredStrategy() object, which supports synchronous distributed training on multiple GPUs on one machine. Then, we move the creation and compliation of the Keras model inside the strategy.scope(). Note that each variable in the model is mirrored across all the replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 10 steps\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 5s 529ms/step - loss: 0.6940\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6965\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6950\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6959\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6949\n"
     ]
    }
   ],
   "source": [
    "# this is the distribution strategy\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# this piece of code is disctributed to multiple GPUs\n",
    "with distribution.scope():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.SGD(0.2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    model.summary()\n",
    "    \n",
    "    # Optimize in the usual way but in reality you are using GPUs.\n",
    "    model.fit(dataset, epochs=5, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each batch of the given input is divided equally among the multiple GPUs. For instance, if using MirroredStrategy() with two GPUs, each batch of size 256 will be divided among the two GPUs, with each of them receiving 128 input examples for each step. In addition, note that each GPU will optimize on the received batches and the TensorFlow backend will combine all these independent optimizations on our behalf. If you want to know more, you can have a look to the notebook online (https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat#scrollTo=wYar3A0vBVtZ) where I explain how to use GPUs in Colab with a Keras model built for MNIST classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiWorkerMirroredStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy implements synchronous distributed training across multiple workers, each one with potentially multiple GPUs. This strategy should be used if you are aiming at scaling beyond a single machine with high performance. Data must be loaded with tf.Dataset and shared across workers so that each worker can read a unique subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPUStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy implements synchronous distributed training on TPUs. TPUs are Google's specialized ASICs chips designed to significantly accelerate machine learning workloads in a way often mor effienct than GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParameterServerStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy implements either multi-GPU synchronous local training or asynchronous multi-machine training. For local training on one machine, the variables of the models are placed on the CPU and operations are replicated across all local GPUs.<br>\n",
    "For multi-machine training, some machines are designated as workers and some as parameter servers with the variables of the model placed on parameter servers. Computation is replicated across all GPUs of all workers. Multiple workers can be set up with the environment variable TF_CONFIG as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    \"cluster\":{\n",
    "        \"worker\":[\"host1:port\", \"host2:port\", \"host3:port\"],\n",
    "        \"ps\":[\"host4:port\", \"host5:port\"]\n",
    "    },\n",
    "    \"task\":{\"type\":\"worker\", \"index\":1}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes in namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>tf.keras.layers : Contains all symbols that were previously under tf.layers\n",
    "    <li>tf.keras.losses : Contains all symbols that were previously under tf.losses\n",
    "    <li>tf.keras.metrics : Contains all symbols that were previously under tf.metrics\n",
    "    <li>tf.debugging : A new namespace for debugging\n",
    "    <li>tf.dtypes : A new namespace for data types\n",
    "    <li>tf.io : A new namespace for I/O\n",
    "    <li>tf.quantization : A new namespace for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from 1.x to 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single file\n",
    "tf_upgrade_v2 --infile tensorfoo.py -- outfile tensorfoo-upgraded.py\n",
    "# for multiple files in a directory\n",
    "tf_upgrade_v2 --intree incode --outtree code-upgraded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorFlow 2.x effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Default to higher-level APIs such as tf.keras (or in certain situations, Estimators) and avoid lower-level APIs\n",
    "<br><br>\n",
    "2. Add a tf.function decorator to make it run efficiently in graph mode with AutoGraph. Only use tf.function to decorate high-level computations; all functions invoked by high-level computations are automatically annotated on your behalf. In this way, you ge the best of both worlds: high-level APIs with eager support, and the efficiency of computational graphs.\n",
    "<br><br>\n",
    "3. Use Python objects to track variables and losses. So, be Pythonic and use tf.Variable instead of tf.get_variable. In this way, variables will be treated with the normal Python scope.\n",
    "<br><br>\n",
    "4. Use tf.data datasets for data inputs and provide these objects directly to tf.keras.Model.fit. In this way, you will have a collection of high-performance classes for manipulating data and will adopt the best way to stream training data from disk.\n",
    "<br><br>\n",
    "5. Use tf.layers modules to combine predifined \"lego bricks\" whenever it is possible, either with Sequential of Functional APIs, or with Subclassing.\n",
    "<br><br>\n",
    "6. Consider using a distribution strategy across GPUs, CPUs, and multiple servers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
